# 정규화모델

### Good Model

    - 현재 데이터(training data)를 잘 설명하는 모델 
    => Explanatory Modeling (Training error를 minimize하는 모델)
    
    - 미래 데이터(testing data)에 대한 예측 성능이 좋은 모델 
    => Predictive Modeling
    
![image](https://user-images.githubusercontent.com/79880336/111635769-32b36e00-883b-11eb-8f06-6f338a08cfae.png)

### Good Predictive Model

![image](https://user-images.githubusercontent.com/79880336/111635785-3646f500-883b-11eb-974a-b4b2d085770d.png)

![image](https://user-images.githubusercontent.com/79880336/111635799-39da7c00-883b-11eb-9ce6-5e5e594f98fa.png)

Expected MSE를 줄이려면 bias, variance 혹은 둘다 낮춰야함
그렇지 못하다면 둘 중에 하나라도 작으면 좋음
Bias가 증가되더라도 variance 감소폭이 더 크다면 expected MSE는 감소(예측 성능 증가)

![image](https://user-images.githubusercontent.com/79880336/111635901-524a9680-883b-11eb-8884-5f3a8ab9c969.png)

### Subset Selection (참고)
1) Subset selection method는 전체 p개의 설명변수(X) 중 일부 k개만을 사용하여 회귀 계수 beta를 추정하는 방법
2) 전체 변수 중 일부만을 선택함에 따라 bias가 증가할 수 있지만 variance가 감소함
        - Best subset selection
        - Forward stepwise selection
        - Backward stepwise elimination
        - Least angle regression
        - Orthogonal matching pursuit


## Regularization(정규화) Concept

![image](https://user-images.githubusercontent.com/79880336/111635946-5d9dc200-883b-11eb-8c9d-eace15d1337d.png)

![image](https://user-images.githubusercontent.com/79880336/111635965-61c9df80-883b-11eb-9010-1eacc721e3c2.png)

### 추가확인
![image](https://user-images.githubusercontent.com/79880336/111639343-82dfff80-883e-11eb-8694-924f43217d69.png)

변수 X,Y에 대해 데이터의 값에 대한 선형모형을 그려보았을 때
맨 왼쪽의 경우는 선형에서 벗어난 값들이 많이 관찰됩니다. (높은 Bias)
이 경우네는 과소적합(Underfitting)되었다고 할 수 있습니다.

반대로 맨 오른쪽의 모형을 보면 자료 대부분의 값이 연결되어 얼핏보면 적합이 잘되어보입니다.

그러나 우리가 만들고자 하는 것은 새로운 데이터를 예측하는 모델이기 때문에 새로운 데이터가 입력되었을 때는 분산이 매운 크게 나타나게 됩니다. (높은 Variance)
이 경우를 과적합(Overfitting) 되어다고 말합니다.


Bias를 약간 희생하면서 Variance를 낮춘 모델인 가운데의 모형이 최적의 예측 모형이 됩니다.

이러한 관계는 아래 그래프로 확인할 수 있습니다.

![image](https://user-images.githubusercontent.com/79880336/111638919-2086ff00-883e-11eb-9fa4-2bd2be95be06.png)

이렇게 Total Error를 가장 낮출수 있는 모형을 찾는 과정을 정규화라고 합니다.

## Regularization Method
1) Regularization method는 회귀 계수 beta가 가질 수 있는 값에 제약조건을 부여하는 방법
2) 제약조건에 의해 beta가 증가할 수 있지만 variance가 감소함

![image](https://user-images.githubusercontent.com/79880336/111636018-6ee6ce80-883b-11eb-9a04-8bc09ed6acfa.png)

Least square Method는 제약조건없이 bias에만 집중해서 beta_1, beta_2의 최솟값을 찾겠다는 의도이지만,

Regularized Method는 제약조건을 두고 beta_1, beta_2를 찾음으로써 bias와 variance 둘다 낮춰보겠다는 의도

## Ridge Regression

Total Error를 낮추기 위해, 다시 말해서 overifitting을 줄이기 위해서는 계수에 제약 조건을 가해줍니다.

![image](https://user-images.githubusercontent.com/79880336/111642175-3649f380-8841-11eb-90e2-907c197c1168.png)

두개의 회귀계수 베타3, 베타4 값을 줄이는 방법을 Ridge Regression 이라고 합니다.

### Ridge Regression 의 회귀식을 일반화

![image](https://user-images.githubusercontent.com/79880336/111642303-4feb3b00-8841-11eb-95d4-86be91de03e3.png)

Ridge Regression에서 람다값이 너무 크다면 모든 회귀계수가 0에 수렴하게 되면 Underfitting 문제가 발생

반대로 람다값이 너무 자가다면 회귀계수에 영향을 미치지 못하게 되어 Overfitting 문제가 발생

따라서 그 사이의 가장 적합한 람다값을 찾아주는 과정이 필요합니다.

![image](https://user-images.githubusercontent.com/79880336/111636052-77d7a000-883b-11eb-9c2c-86fae664f949.png)

## Lasso Regression

![image](https://user-images.githubusercontent.com/79880336/111643776-90978400-8842-11eb-975e-547425ff3755.png)

![image](https://user-images.githubusercontent.com/79880336/111643231-22eb5800-8842-11eb-9d02-2e66d47753a0.png)

### Lasso Parameter

![image](https://user-images.githubusercontent.com/79880336/111644192-f126c100-8842-11eb-9fc7-7f235fa69ab0.png)

![image](https://user-images.githubusercontent.com/79880336/111644232-fc79ec80-8842-11eb-9b0f-58609efa6434.png)

## Ridge, Lasso Regerssion 차이

Ridge Regression은 변수간 상관관계가 높은 상황에서 좋은 예측 성능을 보이는 장점이 있습니다.

그러나 모든 변수를 사용하기때문에 변수의 수가 아주 많을 때는 모델 성능 저하에 영향을 미치는 단점이 있습니다.

Lasso Regression은 많은 변수에서 사용할 수 있는 장점이 있지만,
만약 변수간 상관관계가 높은 변수들이 많다면 
단 한개의 변수만 채택되고 다른 변수들의 계수는 0이 되어 정보의 손실에 따른 정확성의 저하가 올 수 있는 단점이 있습니다.

따라서 상황에 따라 두가지 방법을 잘 선택하여 예측 모델을 만드는 것이 중요합니다.
또한 이 두가지를 보완한 Elastic Net Regression 이라는 분석방법도 존재합니다.


