## lab 10 :
1. Sigmoid 보다 ReLU가 더 좋음 (Better non-linearity) 

 : ReLu (Rectified Linear Unit)
 
 
 
 2 Weight 최기화를 잘 해야함. 
 
![image](https://user-images.githubusercontent.com/79880336/133272184-0426eee1-bfc4-4916-a987-b29a280f1bed.png)

: 초기값에 따라서 달라짐

: Need to set the initial weight values wisely
- Not all 0's
- Challenging issue
- Deep Belief Nets : Restricted Boatman Machine(RBM)



## 최종
![image](https://user-images.githubusercontent.com/79880336/133267649-bf199986-e5e4-499c-9a21-1f5fe215b1c9.png)
