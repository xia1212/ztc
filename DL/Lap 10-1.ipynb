{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57dd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lap 10 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062a574",
   "metadata": {},
   "source": [
    "## SOFTMAX classifier for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108c6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "손으로 쓰는 글씨 (7-2 참조)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70de52d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 7 Learning rate and Evaluation\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58d27d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-16edf28d9adf>:3: read_data_sets (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as: tensorflow_datasets.load('mnist')\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda4\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py:296: _maybe_download (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda4\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py:299: _extract_images (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda4\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py:304: _extract_labels (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda4\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py:112: _dense_to_one_hot (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda4\\lib\\site-packages\\tensorflow\\examples\\tutorials\\mnist\\input_data.py:328: _DataSet.__init__ (from tensorflow.examples.tutorials.mnist.input_data) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/_DataSet.py from tensorflow/models.\n",
      "Epoch: 0001, Cost: 5.608540360\n",
      "Epoch: 0002, Cost: 1.744397119\n",
      "Epoch: 0003, Cost: 1.146677368\n",
      "Epoch: 0004, Cost: 0.906961349\n",
      "Epoch: 0005, Cost: 0.772725626\n",
      "Epoch: 0006, Cost: 0.686069610\n",
      "Epoch: 0007, Cost: 0.623745637\n",
      "Epoch: 0008, Cost: 0.576358337\n",
      "Epoch: 0009, Cost: 0.539237733\n",
      "Epoch: 0010, Cost: 0.509173525\n",
      "Epoch: 0011, Cost: 0.484323362\n",
      "Epoch: 0012, Cost: 0.462864556\n",
      "Epoch: 0013, Cost: 0.445243607\n",
      "Epoch: 0014, Cost: 0.429656731\n",
      "Epoch: 0015, Cost: 0.415969375\n",
      "Epoch: 0016, Cost: 0.404437341\n",
      "Epoch: 0017, Cost: 0.394182904\n",
      "Epoch: 0018, Cost: 0.384080989\n",
      "Epoch: 0019, Cost: 0.376105387\n",
      "Epoch: 0020, Cost: 0.368344888\n",
      "Epoch: 0021, Cost: 0.361322252\n",
      "Epoch: 0022, Cost: 0.354833946\n",
      "Epoch: 0023, Cost: 0.348793914\n",
      "Epoch: 0024, Cost: 0.343194181\n",
      "Epoch: 0025, Cost: 0.337769829\n",
      "Epoch: 0026, Cost: 0.333404447\n",
      "Epoch: 0027, Cost: 0.329409244\n",
      "Epoch: 0028, Cost: 0.324536714\n",
      "Epoch: 0029, Cost: 0.320488803\n",
      "Epoch: 0030, Cost: 0.316843228\n",
      "Epoch: 0031, Cost: 0.313284525\n",
      "Epoch: 0032, Cost: 0.310012747\n",
      "Epoch: 0033, Cost: 0.306709459\n",
      "Epoch: 0034, Cost: 0.303782655\n",
      "Epoch: 0035, Cost: 0.301354194\n",
      "Epoch: 0036, Cost: 0.298247055\n",
      "Epoch: 0037, Cost: 0.295528617\n",
      "Epoch: 0038, Cost: 0.293325746\n",
      "Epoch: 0039, Cost: 0.290757131\n",
      "Epoch: 0040, Cost: 0.288983519\n",
      "Epoch: 0041, Cost: 0.286983423\n",
      "Epoch: 0042, Cost: 0.284863032\n",
      "Epoch: 0043, Cost: 0.282663734\n",
      "Epoch: 0044, Cost: 0.280998944\n",
      "Epoch: 0045, Cost: 0.279440059\n",
      "Epoch: 0046, Cost: 0.277436648\n",
      "Epoch: 0047, Cost: 0.275980054\n",
      "Epoch: 0048, Cost: 0.274120817\n",
      "Epoch: 0049, Cost: 0.273025931\n",
      "Epoch: 0050, Cost: 0.271172021\n",
      "Learning Finished!\n",
      "Accuracy: 0.9167\n",
      "Label:  [5]\n",
      "Prediction:  [5]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W = tf.Variable(tf.random_normal([784, 10]))\n",
    "b = tf.Variable(tf.random_normal([10]))\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "num_iterations = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "hypothesis = tf.matmul(X, W) + b\n",
    "\n",
    "## 아래 4줄이 핵심\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
    "    )\n",
    ")\n",
    "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "# train my model\n",
    "with tf.Session() as sess:\n",
    "    # initialize\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        avg_cost = 0\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            avg_cost += cost_val / num_iterations\n",
    "\n",
    "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
    "\n",
    "    print(\"Learning Finished!\")\n",
    "\n",
    "    # Test model and check accuracy\n",
    "    print(\n",
    "        \"Accuracy:\",\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
    "    )\n",
    "\n",
    "    # Get one and predict\n",
    "    r = random.randint(0, mnist.test.num_examples - 1)\n",
    "\n",
    "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], axis=1)))\n",
    "    print(\n",
    "        \"Prediction: \",\n",
    "        sess.run(\n",
    "            tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r : r + 1]}\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    plt.imshow(\n",
    "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
    "        cmap=\"Greys\",\n",
    "        interpolation=\"nearest\",\n",
    "    )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0443ccd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff07bafe",
   "metadata": {},
   "source": [
    "## NN for MNIST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054e898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 10 MNIST and NN\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71327366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost = 153.653631651\n",
      "Epoch: 0002 cost = 39.708631040\n",
      "Epoch: 0003 cost = 25.358144393\n",
      "Epoch: 0004 cost = 17.928132091\n",
      "Epoch: 0005 cost = 13.165418194\n",
      "Epoch: 0006 cost = 9.812155527\n",
      "Epoch: 0007 cost = 7.448235783\n",
      "Epoch: 0008 cost = 5.624792496\n",
      "Epoch: 0009 cost = 4.249311389\n",
      "Epoch: 0010 cost = 3.248087996\n",
      "Epoch: 0011 cost = 2.423477126\n",
      "Epoch: 0012 cost = 1.836817085\n",
      "Epoch: 0013 cost = 1.432803434\n",
      "Epoch: 0014 cost = 1.105960801\n",
      "Epoch: 0015 cost = 0.861879608\n",
      "Learning Finished!\n",
      "Accuracy: 0.9444\n",
      "Label:  [2]\n",
      "Prediction:  [2]\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "## Relu 사용\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# weights & bias for nn layers\n",
    "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645aa538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "099d77f2",
   "metadata": {},
   "source": [
    "## Xavier for MNIST  (초기화를 잘해야한다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ddd904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 10 MNIST and Xavier\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7aa9efc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.14 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0rc0, 2.3.0rc1, 2.3.0rc2, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0rc0, 2.4.0rc1, 2.4.0rc2, 2.4.0rc3, 2.4.0rc4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.5.0rc0, 2.5.0rc1, 2.5.0rc2, 2.5.0rc3, 2.5.0, 2.5.1, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0)\n",
      "ERROR: No matching distribution found for tensorflow-gpu==1.14\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow-gpu==1.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "339bf0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v1' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d8e85da7378e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m W1 = tf.get_variable(\"W1\", shape=[784, 256],\n\u001b[1;32m---> 21\u001b[1;33m                      initializer=tf.contrib.layers.xavier_initializer())\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mL1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.compat.v1' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "## ReLu 사용\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "\n",
    "## xavier_initializer 사용\n",
    "\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([256]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([256]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L2, W3) + b3\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()\n",
    "\n",
    "## 초기화를 잘해야 정확도가 잘 나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58d5e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf 2버젼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a5f4a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "(60000, 28, 28)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 12.3667 - acc: 0.2318\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 12.5811 - acc: 0.2192\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 12.5460 - acc: 0.2215\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 12.0316 - acc: 0.2535\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 3s 50us/sample - loss: 12.1454 - acc: 0.2464\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 12.6462 - acc: 0.2154\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 11.8672 - acc: 0.2637\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 3s 55us/sample - loss: 11.9214 - acc: 0.2603\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 11.8013 - acc: 0.2678\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 3s 54us/sample - loss: 11.8658 - acc: 0.2638\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 12.0105 - acc: 0.2548\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 12.1566 - acc: 0.2458\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 11.9117 - acc: 0.2609\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 11.9362 - acc: 0.2594\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 3s 52us/sample - loss: 11.9377 - acc: 0.2594\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda4\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "index:  3757 actual y:  8 predicted y:  2\n",
      "index:  7304 actual y:  5 predicted y:  5\n",
      "index:  7300 actual y:  7 predicted y:  5\n",
      "index:  6039 actual y:  9 predicted y:  5\n",
      "index:  9429 actual y:  3 predicted y:  5\n",
      "index:  4420 actual y:  5 predicted y:  5\n",
      "index:  5507 actual y:  2 predicted y:  6\n",
      "index:  8809 actual y:  1 predicted y:  2\n",
      "index:  654 actual y:  5 predicted y:  5\n",
      "index:  7302 actual y:  8 predicted y:  5\n",
      "loss:  11.97574490661621\n",
      "accuracy 0.257\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 MNIST and NN\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(777)  # for reproducibility\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "training_epochs = 15\n",
    "nb_classes = 10\n",
    "\n",
    "(x_train, y_train), (x_test2, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(x_train.shape)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28 * 28)\n",
    "x_test = x_test2.reshape(x_test2.shape[0], 28 * 28)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "# Glorot normal initializer, also called Xavier normal initializer.\n",
    "# see https://www.tensorflow.org/api_docs/python/tf/initializers\n",
    "\n",
    "tf.model.add(tf.keras.layers.Dense(input_dim=784, units=256, kernel_initializer='glorot_normal', activation='relu'))\n",
    "tf.model.add(tf.keras.layers.Dense(units=256, kernel_initializer='glorot_normal', activation='relu'))\n",
    "tf.model.add(tf.keras.layers.Dense(units=nb_classes, kernel_initializer='glorot_normal', activation='softmax'))\n",
    "tf.model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "tf.model.summary()\n",
    "\n",
    "history = tf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n",
    "\n",
    "# predict 10 random hand-writing data\n",
    "y_predicted = tf.model.predict(x_test)\n",
    "for x in range(0, 10):\n",
    "    random_index = random.randint(0, x_test.shape[0]-1)\n",
    "    print(\"index: \", random_index,\n",
    "          \"actual y: \", np.argmax(y_test[random_index]),\n",
    "          \"predicted y: \", np.argmax(y_predicted[random_index]))\n",
    "\n",
    "# evaluate test set\n",
    "evaluation = tf.model.evaluate(x_test, y_test)\n",
    "print('loss: ', evaluation[0])\n",
    "print('accuracy', evaluation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448dddb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21ee59e1",
   "metadata": {},
   "source": [
    "## Deep NN for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ca8a69c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,195,018\n",
      "Trainable params: 1,195,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/15\n",
      "60000/60000 [==============================] - 13s 214us/sample - loss: 13.5844 - acc: 0.1568\n",
      "Epoch 2/15\n",
      "60000/60000 [==============================] - 13s 209us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 3/15\n",
      "60000/60000 [==============================] - 13s 212us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 4/15\n",
      "60000/60000 [==============================] - 13s 215us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 5/15\n",
      "60000/60000 [==============================] - 13s 212us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 6/15\n",
      "60000/60000 [==============================] - 13s 215us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 7/15\n",
      "60000/60000 [==============================] - 13s 209us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 8/15\n",
      "60000/60000 [==============================] - 13s 209us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 9/15\n",
      "60000/60000 [==============================] - 13s 210us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 10/15\n",
      "60000/60000 [==============================] - 13s 213us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 11/15\n",
      "60000/60000 [==============================] - 12s 204us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 12/15\n",
      "60000/60000 [==============================] - 12s 208us/sample - loss: 14.5487 - acc: 0.0974- loss: 14.\n",
      "Epoch 13/15\n",
      "60000/60000 [==============================] - 12s 208us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 14/15\n",
      "60000/60000 [==============================] - 12s 201us/sample - loss: 14.5487 - acc: 0.0974\n",
      "Epoch 15/15\n",
      "60000/60000 [==============================] - 13s 216us/sample - loss: 14.5487 - acc: 0.0974\n",
      "index:  3757 actual y:  8 predicted y:  4\n",
      "index:  7304 actual y:  5 predicted y:  4\n",
      "index:  7300 actual y:  7 predicted y:  4\n",
      "index:  6039 actual y:  9 predicted y:  4\n",
      "index:  9429 actual y:  3 predicted y:  4\n",
      "index:  4420 actual y:  5 predicted y:  4\n",
      "index:  5507 actual y:  2 predicted y:  4\n",
      "index:  8809 actual y:  1 predicted y:  4\n",
      "index:  654 actual y:  5 predicted y:  4\n",
      "index:  7302 actual y:  8 predicted y:  4\n",
      "loss:  14.535298265075683\n",
      "accuracy 0.0982\n"
     ]
    }
   ],
   "source": [
    "# Lab 10 MNIST and NN\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(777)  # for reproducibility\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "training_epochs = 15\n",
    "nb_classes = 10\n",
    "\n",
    "(x_train, y_train), (x_test2, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(x_train.shape)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28 * 28)\n",
    "x_test = x_test2.reshape(x_test2.shape[0], 28 * 28)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "# Glorot normal initializer, also called Xavier normal initializer.\n",
    "# see https://www.tensorflow.org/api_docs/python/tf/initializers\n",
    "\n",
    "tf.model.add(tf.keras.layers.Dense(input_dim=784, units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
    "tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
    "tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
    "tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
    "tf.model.add(tf.keras.layers.Dense(units=nb_classes, kernel_initializer='glorot_normal', activation='softmax'))\n",
    "tf.model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "tf.model.summary()\n",
    "\n",
    "history = tf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n",
    "\n",
    "# predict 10 random hand-writing data\n",
    "y_predicted = tf.model.predict(x_test)\n",
    "for x in range(0, 10):\n",
    "    random_index = random.randint(0, x_test.shape[0]-1)\n",
    "    print(\"index: \", random_index,\n",
    "          \"actual y: \", np.argmax(y_test[random_index]),\n",
    "          \"predicted y: \", np.argmax(y_predicted[random_index]))\n",
    "\n",
    "# evaluate test set\n",
    "evaluation = tf.model.evaluate(x_test, y_test)\n",
    "print('loss: ', evaluation[0])\n",
    "print('accuracy', evaluation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4ef1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d7bf03a",
   "metadata": {},
   "source": [
    "## Dropout for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0d0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 10 MNIST and Dropout\n",
    "import tensorflow as tf\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "tf.set_random_seed(777)  # reproducibility\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
    "# more information about the mnist dataset\n",
    "\n",
    "# parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "\n",
    "# input place holders\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# weights & bias for nn layers\n",
    "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
    "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b1 = tf.Variable(tf.random_normal([512]))\n",
    "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
    "\n",
    "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b2 = tf.Variable(tf.random_normal([512]))\n",
    "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
    "\n",
    "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b3 = tf.Variable(tf.random_normal([512]))\n",
    "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
    "\n",
    "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b4 = tf.Variable(tf.random_normal([512]))\n",
    "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
    "\n",
    "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
    "                     initializer=tf.contrib.layers.xavier_initializer())\n",
    "b5 = tf.Variable(tf.random_normal([10]))\n",
    "hypothesis = tf.matmul(L4, W5) + b5\n",
    "\n",
    "# define cost/loss & optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=hypothesis, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# initialize\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "## 학습단계\n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
    "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "        avg_cost += c / total_batch\n",
    "\n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
    "\n",
    "print('Learning Finished!')\n",
    "\n",
    "# Test model and check accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
    "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
    "\n",
    "# Get one and predict\n",
    "r = random.randint(0, mnist.test.num_examples - 1)\n",
    "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
    "print(\"Prediction: \", sess.run(\n",
    "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
    "\n",
    "## keep_prob 1로 설정하는거 중요\n",
    "\n",
    "\n",
    "# plt.imshow(mnist.test.images[r:r + 1].\n",
    "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e093d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf2 버젼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430537fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 10 MNIST and NN\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(777)  # for reproducibility\n",
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "training_epochs = 15\n",
    "nb_classes = 10\n",
    "drop_rate = 0.3\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "print(x_train.shape)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 28 * 28)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28 * 28)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "# Glorot normal initializer, also called Xavier normal initializer.\n",
    "# see https://www.tensorflow.org/api_docs/python/tf/initializers\n",
    "\n",
    "tf.model.add(tf.keras.layers.Dense(input_dim=784, units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
    "tf.model.add(tf.keras.layers.Dropout(drop_rate))\n",
    "tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
    "tf.model.add(tf.keras.layers.Dropout(drop_rate))\n",
    "tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
    "tf.model.add(tf.keras.layers.Dropout(drop_rate))\n",
    "tf.model.add(tf.keras.layers.Dense(units=512, kernel_initializer='glorot_normal', activation='relu'))\n",
    "tf.model.add(tf.keras.layers.Dropout(drop_rate))\n",
    "tf.model.add(tf.keras.layers.Dense(units=nb_classes, kernel_initializer='glorot_normal', activation='softmax'))\n",
    "tf.model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "tf.model.summary()\n",
    "\n",
    "history = tf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n",
    "\n",
    "# predict 10 random hand-writing data\n",
    "y_predicted = tf.model.predict(x_test)\n",
    "for x in range(0, 10):\n",
    "    random_index = random.randint(0, x_test.shape[0]-1)\n",
    "    print(\"index: \", random_index,\n",
    "          \"actual y: \", np.argmax(y_test[random_index]),\n",
    "          \"predicted y: \", np.argmax(y_predicted[random_index]))\n",
    "\n",
    "# evaluate test set\n",
    "evaluation = tf.model.evaluate(x_test, y_test)\n",
    "print('loss: ', evaluation[0])\n",
    "print('accuracy', evaluation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d889a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
