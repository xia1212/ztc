{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c3ba3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lab 11 MNIST and Deep learning CNN\n",
    "# https://www.tensorflow.org/tutorials/layers\n",
    "import numpy as np\n",
    "\n",
    "#import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "tf.set_random_seed(777)  # for reproducibility\n",
    "\n",
    "data_train, data_test = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "261a6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-50fb54506379>:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "WARNING:tensorflow:From <ipython-input-4-50fb54506379>:39: conv2d (from tensorflow.python.keras.legacy_tf_layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda4\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-50fb54506379>:42: max_pooling2d (from tensorflow.python.keras.legacy_tf_layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-50fb54506379>:44: dropout (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "WARNING:tensorflow:From <ipython-input-4-50fb54506379>:65: dense (from tensorflow.python.keras.legacy_tf_layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "Learning Started!\n",
      "Epoch: 0001 cost = [1.34894056 1.11282098]\n",
      "Epoch: 0002 cost = [0.214438   0.18758906]\n",
      "Epoch: 0003 cost = [0.14752462 0.13848489]\n",
      "Epoch: 0004 cost = [0.12289532 0.11031106]\n",
      "Epoch: 0005 cost = [0.10409465 0.09700904]\n",
      "Epoch: 0006 cost = [0.09344966 0.08862305]\n",
      "Epoch: 0007 cost = [0.08973733 0.08533802]\n",
      "Epoch: 0008 cost = [0.08191774 0.08052291]\n",
      "Epoch: 0009 cost = [0.07812401 0.08250321]\n",
      "Epoch: 0010 cost = [0.07770228 0.07806969]\n",
      "Epoch: 0011 cost = [0.07332602 0.07480765]\n",
      "Epoch: 0012 cost = [0.07282083 0.07065851]\n",
      "Epoch: 0013 cost = [0.07315348 0.06855809]\n",
      "Epoch: 0014 cost = [0.07363027 0.06918367]\n",
      "Epoch: 0015 cost = [0.06982953 0.06945991]\n",
      "Epoch: 0016 cost = [0.07016908 0.06694381]\n",
      "Epoch: 0017 cost = [0.06783362 0.06489236]\n",
      "Epoch: 0018 cost = [0.06719208 0.06748781]\n",
      "Epoch: 0019 cost = [0.06680612 0.06424414]\n",
      "Epoch: 0020 cost = [0.06312228 0.06733979]\n",
      "Learning Finished!\n",
      "0 Accuracy: 0.9914\n",
      "1 Accuracy: 0.9925\n",
      "Ensemble accuracy: 0.9927\n"
     ]
    }
   ],
   "source": [
    "# Parse images and labels\n",
    "(images_train, labels_train) = data_train\n",
    "labels_train = tf.keras.utils.to_categorical(labels_train,10)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((images_train, labels_train))\n",
    " \n",
    "(images_test, labels_test) = data_test\n",
    "labels_test = tf.keras.utils.to_categorical(labels_test,10)\n",
    " \n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    " \n",
    "train_dataset = train_dataset.repeat().batch(batch_size).prefetch(1)\n",
    "train_iterator = train_dataset.make_one_shot_iterator()\n",
    "next_batch_train = train_iterator.get_next()\n",
    " \n",
    "class Model:\n",
    " \n",
    "    def __init__(self, sess, name):\n",
    "        self.sess = sess\n",
    "        self.name = name\n",
    "        self._build_net()\n",
    " \n",
    "    def _build_net(self):\n",
    "        with tf.variable_scope(self.name):\n",
    "            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n",
    "            # for testing\n",
    "            self.training = tf.placeholder(tf.bool)\n",
    " \n",
    "            # input place holders\n",
    "            self.X = tf.placeholder(tf.float32, [None, 28, 28])\n",
    "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
    "            \n",
    "            # img 28x28x1 (black/white), Input Layer\n",
    "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
    " \n",
    "            # Convolutional Layer #1\n",
    "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            # Pooling Layer #1\n",
    "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
    "                                         rate=0.3, training=self.training)\n",
    " \n",
    "            # Convolutional Layer #2 and Pooling Layer #2\n",
    "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
    "                                         rate=0.3, training=self.training)\n",
    " \n",
    "            # Convolutional Layer #3 and Pooling Layer #3\n",
    "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n",
    "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
    "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
    "                                            padding=\"SAME\", strides=2)\n",
    "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
    "                                         rate=0.3, training=self.training)\n",
    " \n",
    "            # Dense Layer with Relu\n",
    "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
    "            dense4 = tf.layers.dense(inputs=flat,\n",
    "                                     units=625, activation=tf.nn.relu)\n",
    "            dropout4 = tf.layers.dropout(inputs=dense4,\n",
    "                                         rate=0.5, training=self.training)\n",
    " \n",
    "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
    "            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n",
    " \n",
    "        # define cost/loss & optimizer\n",
    "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=self.logits, labels=self.Y))\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=learning_rate).minimize(self.cost)\n",
    " \n",
    "        correct_prediction = tf.equal(\n",
    "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    " \n",
    "    def predict(self, x_test, training=False):\n",
    "        return self.sess.run(self.logits,\n",
    "                             feed_dict={self.X: x_test, self.training: training})\n",
    " \n",
    "    def get_accuracy(self, x_test, y_test, training=False):\n",
    "        return self.sess.run(self.accuracy,\n",
    "                             feed_dict={self.X: x_test,\n",
    "                                        self.Y: y_test, self.training: training})\n",
    " \n",
    "    def train(self, x_data, y_data, training=True):\n",
    "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
    "            self.X: x_data, self.Y: y_data, self.training: training})\n",
    " \n",
    "# initialize\n",
    "sess = tf.Session()\n",
    " \n",
    "models = []\n",
    "num_models = 2\n",
    "for m in range(num_models):\n",
    "    models.append(Model(sess, \"model\" + str(m)))\n",
    " \n",
    "sess.run(tf.global_variables_initializer())\n",
    " \n",
    "print('Learning Started!')\n",
    " \n",
    "# train my model\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost_list = np.zeros(len(models))\n",
    "    total_batch = int(len(labels_train) / batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = sess.run(next_batch_train)\n",
    " \n",
    "        # train each model\n",
    "        for m_idx, m in enumerate(models):\n",
    "            c, _ = m.train(batch_xs, batch_ys)\n",
    "            avg_cost_list[m_idx] += c / total_batch\n",
    " \n",
    "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
    " \n",
    "print('Learning Finished!')\n",
    " \n",
    "# Test model and check accuracy\n",
    "test_size = len(labels_test)\n",
    "predictions = np.zeros([test_size, 10])\n",
    "for m_idx, m in enumerate(models):\n",
    "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
    "        images_test, labels_test))\n",
    "    p = m.predict(images_test)\n",
    "    predictions += p\n",
    " \n",
    "ensemble_correct_prediction = tf.equal(\n",
    "    tf.argmax(predictions, 1), tf.argmax(labels_test, 1))\n",
    "ensemble_accuracy = tf.reduce_mean(\n",
    "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
    "print('Ensemble accuracy:', sess.run(ensemble_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf2 버젼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1f447c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 16)        160       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 13, 13, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 11, 11, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 5, 5, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                8010      \n",
      "=================================================================\n",
      "Total params: 12,810\n",
      "Trainable params: 12,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 60000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 14s 232us/sample - loss: 0.3820 - acc: 0.8951\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 14s 233us/sample - loss: 0.0953 - acc: 0.9718\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 14s 232us/sample - loss: 0.0682 - acc: 0.9795\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 15s 253us/sample - loss: 0.0560 - acc: 0.9836\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 18s 295us/sample - loss: 0.0487 - acc: 0.9853\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 15s 254us/sample - loss: 0.0431 - acc: 0.9870\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 18s 294us/sample - loss: 0.0390 - acc: 0.9878\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 17s 289us/sample - loss: 0.0357 - acc: 0.9888\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 16s 269us/sample - loss: 0.0329 - acc: 0.9895\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 16s 272us/sample - loss: 0.0306 - acc: 0.9904\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 18s 305us/sample - loss: 0.0265 - acc: 0.9917\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 16s 269us/sample - loss: 0.0262 - acc: 0.9919\n",
      "WARNING:tensorflow:From C:\\Users\\user\\anaconda4\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "index:  5252 actual y:  1 predicted y:  1\n",
      "index:  226 actual y:  8 predicted y:  8\n",
      "index:  7737 actual y:  0 predicted y:  0\n",
      "index:  3233 actual y:  4 predicted y:  4\n",
      "index:  9274 actual y:  1 predicted y:  1\n",
      "index:  1695 actual y:  9 predicted y:  9\n",
      "index:  4974 actual y:  4 predicted y:  4\n",
      "index:  3233 actual y:  4 predicted y:  4\n",
      "index:  2626 actual y:  1 predicted y:  1\n",
      "index:  1142 actual y:  4 predicted y:  4\n",
      "loss:  0.04245725563337328\n",
      "accuracy 0.987\n"
     ]
    }
   ],
   "source": [
    "# Lab 11 MNIST and Convolutional Neural Network\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_test = x_test / 255\n",
    "x_train = x_train / 255\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# one hot encode y data\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# hyper parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 12\n",
    "batch_size = 128\n",
    "\n",
    "tf.model = tf.keras.Sequential()\n",
    "# L1\n",
    "tf.model.add(tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'))\n",
    "tf.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# L2\n",
    "tf.model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "tf.model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# L3 fully connected\n",
    "tf.model.add(tf.keras.layers.Flatten())\n",
    "tf.model.add(tf.keras.layers.Dense(units=10, kernel_initializer='glorot_normal', activation='softmax'))\n",
    "\n",
    "tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n",
    "tf.model.summary()\n",
    "\n",
    "tf.model.fit(x_train, y_train, batch_size=batch_size, epochs=training_epochs)\n",
    "\n",
    "# predict 10 random hand-writing data\n",
    "y_predicted = tf.model.predict(x_test)\n",
    "for x in range(0, 10):\n",
    "    random_index = random.randint(0, x_test.shape[0]-1)\n",
    "    print(\"index: \", random_index,\n",
    "          \"actual y: \", np.argmax(y_test[random_index]),\n",
    "          \"predicted y: \", np.argmax(y_predicted[random_index]))\n",
    "\n",
    "evaluation = tf.model.evaluate(x_test, y_test)\n",
    "print('loss: ', evaluation[0])\n",
    "print('accuracy', evaluation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedce294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
